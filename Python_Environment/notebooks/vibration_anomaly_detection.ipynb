{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detec√ß√£o de Anomalias em M√°quinas Industriais usando Dados de Vibra√ß√£o 3-Eixos\n",
    "\n",
    "Este notebook implementa detec√ß√£o de anomalias para m√°quinas industriais usando dados de vibra√ß√£o de 3 eixos, baseado no exemplo do MATLAB Predictive Maintenance Toolbox.\n",
    "\n",
    "## Metodologia\n",
    "\n",
    "Implementamos tr√™s abordagens diferentes de detec√ß√£o de anomalias:\n",
    "1. **One-Class SVM**: Identifica anormalidades \"distantes\" dos dados normais\n",
    "2. **Isolation Forest**: Usa √°rvores de decis√£o para isolar observa√ß√µes\n",
    "3. **Autoencoder**: Rede neural que reconstr√≥i dados normais com baixo erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"TensorFlow vers√£o: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defini√ß√£o da Classe Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VibrationAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detector de anomalias para m√°quinas industriais usando dados de vibra√ß√£o 3-eixos\n",
    "    Baseado no exemplo do MATLAB Predictive Maintenance Toolbox\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.feature_names = [\n",
    "            'Ch1_CrestFactor', 'Ch1_Kurtosis', 'Ch1_RMS', 'Ch1_StdDev',\n",
    "            'Ch2_Mean', 'Ch2_RMS', 'Ch2_Skewness', 'Ch2_StdDev',\n",
    "            'Ch3_CrestFactor', 'Ch3_SINAD', 'Ch3_SNR', 'Ch3_THD'\n",
    "        ]\n",
    "        print(\"Detector de Anomalias de Vibra√ß√£o inicializado!\")\n",
    "        print(f\"Features extra√≠das: {len(self.feature_names)}\")\n",
    "    \n",
    "    def generate_synthetic_data(self, n_normal=1000, n_anomaly=200):\n",
    "        \"\"\"\n",
    "        Gera dados sint√©ticos de vibra√ß√£o 3-eixos com comportamento normal e an√¥malo\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Dados normais (ap√≥s manuten√ß√£o)\n",
    "        normal_data = []\n",
    "        for i in range(n_normal):\n",
    "            # Features do Canal 1 (padr√µes normais de vibra√ß√£o)\n",
    "            ch1_crest = np.random.normal(3.2, 0.3)  # Fator de Crista\n",
    "            ch1_kurt = np.random.normal(3.1, 0.2)   # Curtose\n",
    "            ch1_rms = np.random.normal(0.15, 0.02)  # RMS\n",
    "            ch1_std = np.random.normal(0.12, 0.015) # Desvio Padr√£o\n",
    "            \n",
    "            # Features do Canal 2\n",
    "            ch2_mean = np.random.normal(0.001, 0.0005)  # M√©dia\n",
    "            ch2_rms = np.random.normal(0.14, 0.018)     # RMS\n",
    "            ch2_skew = np.random.normal(0.05, 0.02)     # Assimetria\n",
    "            ch2_std = np.random.normal(0.11, 0.012)     # Desvio Padr√£o\n",
    "            \n",
    "            # Features do Canal 3\n",
    "            ch3_crest = np.random.normal(3.1, 0.25)  # Fator de Crista\n",
    "            ch3_sinad = np.random.normal(42, 2)      # SINAD\n",
    "            ch3_snr = np.random.normal(38, 1.5)      # SNR\n",
    "            ch3_thd = np.random.normal(0.08, 0.01)   # THD\n",
    "            \n",
    "            normal_data.append([\n",
    "                ch1_crest, ch1_kurt, ch1_rms, ch1_std,\n",
    "                ch2_mean, ch2_rms, ch2_skew, ch2_std,\n",
    "                ch3_crest, ch3_sinad, ch3_snr, ch3_thd\n",
    "            ])\n",
    "        \n",
    "        # Dados an√¥malos (antes da manuten√ß√£o - condi√ß√µes degradadas)\n",
    "        anomaly_data = []\n",
    "        for i in range(n_anomaly):\n",
    "            # Features do Canal 1 (an√¥malas - vibra√ß√µes maiores, padr√µes diferentes)\n",
    "            ch1_crest = np.random.normal(4.8, 0.5)    # Fator de crista maior\n",
    "            ch1_kurt = np.random.normal(5.2, 0.8)     # Curtose maior\n",
    "            ch1_rms = np.random.normal(0.35, 0.08)    # RMS maior\n",
    "            ch1_std = np.random.normal(0.28, 0.05)    # Desvio padr√£o maior\n",
    "            \n",
    "            # Features do Canal 2 (an√¥malas)\n",
    "            ch2_mean = np.random.normal(0.008, 0.003)  # M√©dia maior\n",
    "            ch2_rms = np.random.normal(0.32, 0.06)     # RMS maior\n",
    "            ch2_skew = np.random.normal(0.25, 0.08)    # Assimetria maior\n",
    "            ch2_std = np.random.normal(0.25, 0.04)     # Desvio padr√£o maior\n",
    "            \n",
    "            # Features do Canal 3 (an√¥malas)\n",
    "            ch3_crest = np.random.normal(5.1, 0.6)   # Fator de crista maior\n",
    "            ch3_sinad = np.random.normal(28, 4)      # SINAD menor (mais distor√ß√£o)\n",
    "            ch3_snr = np.random.normal(22, 3)        # SNR menor (mais ru√≠do)\n",
    "            ch3_thd = np.random.normal(0.18, 0.03)   # THD maior (mais distor√ß√£o)\n",
    "            \n",
    "            anomaly_data.append([\n",
    "                ch1_crest, ch1_kurt, ch1_rms, ch1_std,\n",
    "                ch2_mean, ch2_rms, ch2_skew, ch2_std,\n",
    "                ch3_crest, ch3_sinad, ch3_snr, ch3_thd\n",
    "            ])\n",
    "        \n",
    "        # Combinar dados\n",
    "        X_normal = np.array(normal_data)\n",
    "        X_anomaly = np.array(anomaly_data)\n",
    "        \n",
    "        # Criar r√≥tulos (0 = normal, 1 = anomalia)\n",
    "        y_normal = np.zeros(n_normal)\n",
    "        y_anomaly = np.ones(n_anomaly)\n",
    "        \n",
    "        X = np.vstack([X_normal, X_anomaly])\n",
    "        y = np.hstack([y_normal, y_anomaly])\n",
    "        \n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame(X, columns=self.feature_names)\n",
    "        df['label'] = y\n",
    "        df['condition'] = ['Normal' if label == 0 else 'Anomalia' for label in y]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        Prepara dados para treinamento - divide e normaliza\n",
    "        \"\"\"\n",
    "        # Features e r√≥tulos\n",
    "        X = df[self.feature_names].values\n",
    "        y = df['label'].values\n",
    "        \n",
    "        # Dividir dados\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Para detec√ß√£o de anomalias, tipicamente treinamos apenas com dados normais\n",
    "        X_train_normal = X_train[y_train == 0]\n",
    "        \n",
    "        # Normalizar os dados\n",
    "        X_train_normal_scaled = self.scaler.fit_transform(X_train_normal)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_normal_scaled, X_test_scaled, y_test\n",
    "\n",
    "print(\"Classe VibrationAnomalyDetector definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gera√ß√£o e Explora√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o detector\n",
    "detector = VibrationAnomalyDetector()\n",
    "\n",
    "# Gerar dados sint√©ticos\n",
    "print(\"Gerando dados sint√©ticos de vibra√ß√£o...\")\n",
    "df = detector.generate_synthetic_data(n_normal=1000, n_anomaly=200)\n",
    "\n",
    "print(f\"\\nDados gerados:\")\n",
    "print(f\"- Total de amostras: {len(df)}\")\n",
    "print(f\"- Amostras normais: {len(df[df['label']==0])}\")\n",
    "print(f\"- Amostras an√¥malas: {len(df[df['label']==1])}\")\n",
    "print(f\"- Porcentagem de anomalias: {df['label'].mean()*100:.1f}%\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"\\nPrimeiras 5 amostras:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise Explorat√≥ria dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas\n",
    "print(\"Estat√≠sticas descritivas por condi√ß√£o:\")\n",
    "print(\"\\n=== DADOS NORMAIS ===\")\n",
    "display(df[df['condition']=='Normal'][detector.feature_names].describe())\n",
    "\n",
    "print(\"\\n=== DADOS AN√îMALOS ===\")\n",
    "display(df[df['condition']=='Anomalia'][detector.feature_names].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o da distribui√ß√£o dos dados\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(detector.feature_names):\n",
    "    # Histograma para cada feature\n",
    "    df[df['condition']=='Normal'][feature].hist(alpha=0.7, label='Normal', bins=30, ax=axes[i])\n",
    "    df[df['condition']=='Anomalia'][feature].hist(alpha=0.7, label='Anomalia', bins=30, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribui√ß√£o das Features por Condi√ß√£o', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots para visualizar separa√ß√£o entre classes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Canal 1 - RMS vs Curtose\n",
    "axes[0,0].scatter(df[df['condition']=='Normal']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Normal']['Ch1_Kurtosis'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[0,0].scatter(df[df['condition']=='Anomalia']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Anomalia']['Ch1_Kurtosis'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[0,0].set_xlabel('Canal 1 - RMS')\n",
    "axes[0,0].set_ylabel('Canal 1 - Curtose')\n",
    "axes[0,0].set_title('Canal 1: RMS vs Curtose')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Canal 2 - RMS vs Desvio Padr√£o\n",
    "axes[0,1].scatter(df[df['condition']=='Normal']['Ch2_RMS'], \n",
    "                  df[df['condition']=='Normal']['Ch2_StdDev'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[0,1].scatter(df[df['condition']=='Anomalia']['Ch2_RMS'], \n",
    "                  df[df['condition']=='Anomalia']['Ch2_StdDev'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[0,1].set_xlabel('Canal 2 - RMS')\n",
    "axes[0,1].set_ylabel('Canal 2 - Desvio Padr√£o')\n",
    "axes[0,1].set_title('Canal 2: RMS vs Desvio Padr√£o')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Canal 3 - SNR vs THD\n",
    "axes[1,0].scatter(df[df['condition']=='Normal']['Ch3_SNR'], \n",
    "                  df[df['condition']=='Normal']['Ch3_THD'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[1,0].scatter(df[df['condition']=='Anomalia']['Ch3_SNR'], \n",
    "                  df[df['condition']=='Anomalia']['Ch3_THD'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[1,0].set_xlabel('Canal 3 - SNR')\n",
    "axes[1,0].set_ylabel('Canal 3 - THD')\n",
    "axes[1,0].set_title('Canal 3: SNR vs THD')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Correla√ß√£o entre canais\n",
    "axes[1,1].scatter(df[df['condition']=='Normal']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Normal']['Ch2_RMS'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[1,1].scatter(df[df['condition']=='Anomalia']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Anomalia']['Ch2_RMS'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[1,1].set_xlabel('Canal 1 - RMS')\n",
    "axes[1,1].set_ylabel('Canal 2 - RMS')\n",
    "axes[1,1].set_title('Correla√ß√£o entre Canais: RMS')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para treinamento\n",
    "print(\"Preparando dados para treinamento...\")\n",
    "X_train_normal, X_test, y_test = detector.prepare_data(df)\n",
    "\n",
    "print(f\"\\nDivis√£o dos dados:\")\n",
    "print(f\"- Conjunto de treino (apenas normais): {X_train_normal.shape[0]} amostras\")\n",
    "print(f\"- Conjunto de teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"- Features: {X_train_normal.shape[1]}\")\n",
    "print(f\"- Anomalias no teste: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# Verificar normaliza√ß√£o\n",
    "print(f\"\\nVerifica√ß√£o da normaliza√ß√£o (conjunto de treino):\")\n",
    "print(f\"- M√©dia: {X_train_normal.mean(axis=0)[:3]:.4f}... (primeiras 3 features)\")\n",
    "print(f\"- Desvio padr√£o: {X_train_normal.std(axis=0)[:3]:.4f}... (primeiras 3 features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementa√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_class_svm(detector, X_train_normal):\n",
    "    \"\"\"\n",
    "    Treina One-Class SVM para detec√ß√£o de anomalias\n",
    "    \"\"\"\n",
    "    print(\"Treinando One-Class SVM...\")\n",
    "    \n",
    "    # One-Class SVM\n",
    "    oc_svm = OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')\n",
    "    oc_svm.fit(X_train_normal)\n",
    "    \n",
    "    detector.models['OneClassSVM'] = oc_svm\n",
    "    print(\"‚úÖ Treinamento do One-Class SVM conclu√≠do.\")\n",
    "    \n",
    "    # Informa√ß√µes do modelo\n",
    "    print(f\"   - Par√¢metro nu: {oc_svm.nu}\")\n",
    "    print(f\"   - Kernel: {oc_svm.kernel}\")\n",
    "    print(f\"   - Gamma: {oc_svm.gamma}\")\n",
    "    print(f\"   - Vetores de suporte: {len(oc_svm.support_vectors_)}\")\n",
    "\n",
    "# Treinar One-Class SVM\n",
    "train_one_class_svm(detector, X_train_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_isolation_forest(detector, X_train_normal):\n",
    "    \"\"\"\n",
    "    Treina Isolation Forest para detec√ß√£o de anomalias\n",
    "    \"\"\"\n",
    "    print(\"Treinando Isolation Forest...\")\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "    iso_forest.fit(X_train_normal)\n",
    "    \n",
    "    detector.models['IsolationForest'] = iso_forest\n",
    "    print(\"‚úÖ Treinamento do Isolation Forest conclu√≠do.\")\n",
    "    \n",
    "    # Informa√ß√µes do modelo\n",
    "    print(f\"   - Contamina√ß√£o esperada: {iso_forest.contamination}\")\n",
    "    print(f\"   - N√∫mero de √°rvores: {iso_forest.n_estimators}\")\n",
    "    print(f\"   - Max samples: {iso_forest.max_samples}\")\n",
    "\n",
    "# Treinar Isolation Forest\n",
    "train_isolation_forest(detector, X_train_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Autoencoder Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Constr√≥i rede neural autoencoder\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(8, activation='relu', input_shape=(input_dim,), name='encoder_layer1'),\n",
    "        keras.layers.Dense(4, activation='relu', name='encoder_layer2'),\n",
    "        keras.layers.Dense(2, activation='relu', name='bottleneck')  # Camada gargalo\n",
    "    ], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(4, activation='relu', input_shape=(2,), name='decoder_layer1'),\n",
    "        keras.layers.Dense(8, activation='relu', name='decoder_layer2'),\n",
    "        keras.layers.Dense(input_dim, activation='linear', name='output')\n",
    "    ], name='decoder')\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = keras.Sequential([encoder, decoder], name='autoencoder')\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "def train_autoencoder(detector, X_train_normal):\n",
    "    \"\"\"\n",
    "    Treina autoencoder para detec√ß√£o de anomalias\n",
    "    \"\"\"\n",
    "    print(\"Treinando Autoencoder...\")\n",
    "    \n",
    "    # Construir e treinar autoencoder\n",
    "    autoencoder, encoder, decoder = build_autoencoder(X_train_normal.shape[1])\n",
    "    \n",
    "    # Mostrar arquitetura\n",
    "    print(\"\\nArquitetura do Autoencoder:\")\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    # Treinar apenas com dados normais\n",
    "    history = autoencoder.fit(\n",
    "        X_train_normal, X_train_normal,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    detector.models['Autoencoder'] = autoencoder\n",
    "    detector.models['Encoder'] = encoder\n",
    "    detector.models['Decoder'] = decoder\n",
    "    \n",
    "    print(\"‚úÖ Treinamento do Autoencoder conclu√≠do.\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Treinar Autoencoder\n",
    "history = train_autoencoder(detector, X_train_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar hist√≥rico de treinamento do Autoencoder\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Treino', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2)\n",
    "axes[0].set_title('Fun√ß√£o de Perda do Autoencoder')\n",
    "axes[0].set_xlabel('√âpoca')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Treino', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Valida√ß√£o', linewidth=2)\n",
    "axes[1].set_title('Erro Absoluto M√©dio do Autoencoder')\n",
    "axes[1].set_xlabel('√âpoca')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Perda final de treino: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Perda final de valida√ß√£o: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predi√ß√µes e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalies(detector, X_test):\n",
    "    \"\"\"\n",
    "    Prediz anomalias usando todos os modelos treinados\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Predi√ß√µes do One-Class SVM\n",
    "    if 'OneClassSVM' in detector.models:\n",
    "        print(\"Fazendo predi√ß√µes com One-Class SVM...\")\n",
    "        oc_pred = detector.models['OneClassSVM'].predict(X_test)\n",
    "        # Converter -1 (anomalia) para 1, e 1 (normal) para 0\n",
    "        predictions['OneClassSVM'] = (oc_pred == -1).astype(int)\n",
    "        print(f\"  Anomalias detectadas: {predictions['OneClassSVM'].sum()}\")\n",
    "    \n",
    "    # Predi√ß√µes do Isolation Forest\n",
    "    if 'IsolationForest' in detector.models:\n",
    "        print(\"Fazendo predi√ß√µes com Isolation Forest...\")\n",
    "        iso_pred = detector.models['IsolationForest'].predict(X_test)\n",
    "        # Converter -1 (anomalia) para 1, e 1 (normal) para 0\n",
    "        predictions['IsolationForest'] = (iso_pred == -1).astype(int)\n",
    "        print(f\"  Anomalias detectadas: {predictions['IsolationForest'].sum()}\")\n",
    "    \n",
    "    # Predi√ß√µes do Autoencoder\n",
    "    if 'Autoencoder' in detector.models:\n",
    "        print(\"Fazendo predi√ß√µes com Autoencoder...\")\n",
    "        # Reconstruir dados\n",
    "        reconstructed = detector.models['Autoencoder'].predict(X_test, verbose=0)\n",
    "        # Calcular erro de reconstru√ß√£o\n",
    "        mse = np.mean(np.square(X_test - reconstructed), axis=1)\n",
    "        # Usar threshold para classificar (percentil 95 dos erros de reconstru√ß√£o)\n",
    "        threshold = np.percentile(mse, 95)\n",
    "        predictions['Autoencoder'] = (mse > threshold).astype(int)\n",
    "        print(f\"  Threshold de erro: {threshold:.4f}\")\n",
    "        print(f\"  Anomalias detectadas: {predictions['Autoencoder'].sum()}\")\n",
    "        \n",
    "        # Armazenar erros de reconstru√ß√£o para an√°lise\n",
    "        detector.reconstruction_errors = mse\n",
    "        detector.threshold = threshold\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "print(\"Fazendo predi√ß√µes com todos os modelos...\")\n",
    "predictions = predict_anomalies(detector, X_test)\n",
    "print(f\"\\nTotal de anomalias reais no teste: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avalia√ß√£o Detalhada dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_true, predictions):\n",
    "    \"\"\"\n",
    "    Avalia performance dos modelos\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, y_pred in predictions.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"üìä RESULTADOS DO {model_name.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Relat√≥rio de classifica√ß√£o\n",
    "        report = classification_report(y_true, y_pred, target_names=['Normal', 'Anomalia'])\n",
    "        print(report)\n",
    "        \n",
    "        # Matriz de confus√£o\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(f\"\\nMatriz de Confus√£o:\")\n",
    "        print(f\"         Predito\")\n",
    "        print(f\"       Normal Anomalia\")\n",
    "        print(f\"Normal   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "        print(f\"Anomalia {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìà M√©tricas Detalhadas:\")\n",
    "        print(f\"   ‚Ä¢ Acur√°cia: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Precis√£o: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Recall (Sensibilidade): {recall:.3f} ({recall*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Especificidade: {specificity:.3f} ({specificity*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ F1-Score: {f1:.3f}\")\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'specificity': specificity,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Avaliar todos os modelos\n",
    "results = evaluate_models(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualiza√ß√µes dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o visual dos modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Preparar dados para compara√ß√£o\n",
    "model_names = list(results.keys())\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "metric_labels = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']\n",
    "\n",
    "# Gr√°fico de barras comparativo\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    values = [results[model][metric] for model in model_names]\n",
    "    bars = ax.bar(x, values, width=0.6, alpha=0.8)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{label} por Modelo', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confus√£o visual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    cm = result['confusion_matrix']\n",
    "    \n",
    "    # Heatmap da matriz de confus√£o\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Anomalia'],\n",
    "                yticklabels=['Normal', 'Anomalia'],\n",
    "                ax=axes[i], cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    axes[i].set_title(f'{model_name}\\nAcur√°cia: {result[\"accuracy\"]:.3f}', \n",
    "                      fontweight='bold')\n",
    "    axes[i].set_ylabel('Valores Reais')\n",
    "    axes[i].set_xlabel('Predi√ß√µes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise dos erros de reconstru√ß√£o do Autoencoder\n",
    "if hasattr(detector, 'reconstruction_errors'):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histograma dos erros de reconstru√ß√£o\n",
    "    normal_errors = detector.reconstruction_errors[y_test == 0]\n",
    "    anomaly_errors = detector.reconstruction_errors[y_test == 1]\n",
    "    \n",
    "    axes[0].hist(normal_errors, bins=30, alpha=0.7, label='Normal', density=True)\n",
    "    axes[0].hist(anomaly_errors, bins=30, alpha=0.7, label='Anomalia', density=True)\n",
    "    axes[0].axvline(detector.threshold, color='red', linestyle='--', \n",
    "                    label=f'Threshold ({detector.threshold:.4f})')\n",
    "    axes[0].set_xlabel('Erro de Reconstru√ß√£o (MSE)')\n",
    "    axes[0].set_ylabel('Densidade')\n",
    "    axes[0].set_title('Distribui√ß√£o dos Erros de Reconstru√ß√£o')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot dos erros\n",
    "    data_for_box = [normal_errors, anomaly_errors]\n",
    "    axes[1].boxplot(data_for_box, labels=['Normal', 'Anomalia'])\n",
    "    axes[1].axhline(detector.threshold, color='red', linestyle='--', \n",
    "                    label=f'Threshold ({detector.threshold:.4f})')\n",
    "    axes[1].set_ylabel('Erro de Reconstru√ß√£o (MSE)')\n",
    "    axes[1].set_title('Box Plot dos Erros de Reconstru√ß√£o')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Estat√≠sticas dos erros de reconstru√ß√£o:\")\n",
    "    print(f\"Normal - M√©dia: {normal_errors.mean():.4f}, Desvio: {normal_errors.std():.4f}\")\n",
    "    print(f\"Anomalia - M√©dia: {anomaly_errors.mean():.4f}, Desvio: {anomaly_errors.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo Final e Compara√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela resumo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ RESUMO FINAL - COMPARA√á√ÉO DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Criar DataFrame para compara√ß√£o\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'Acur√°cia': f\"{result['accuracy']:.3f}\",\n",
    "        'Precis√£o': f\"{result['precision']:.3f}\",\n",
    "        'Recall': f\"{result['recall']:.3f}\",\n",
    "        'F1-Score': f\"{result['f1_score']:.3f}\",\n",
    "        'Especificidade': f\"{result['specificity']:.3f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# Identificar melhor modelo por m√©trica\n",
    "print(\"\\nüèÜ MELHORES MODELOS POR M√âTRICA:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "    best_model = max(results.keys(), key=lambda x: results[x][metric])\n",
    "    best_value = results[best_model][metric]\n",
    "    metric_name = {'accuracy': 'Acur√°cia', 'precision': 'Precis√£o', \n",
    "                   'recall': 'Recall', 'f1_score': 'F1-Score'}[metric]\n",
    "    print(f\"   ‚Ä¢ {metric_name}: {best_model} ({best_value:.3f})\")\n",
    "\n",
    "# An√°lise interpretativa\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã AN√ÅLISE INTERPRETATIVA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç Caracter√≠sticas dos Modelos:\")\n",
    "print(\"\"\"\n",
    "‚Ä¢ ONE-CLASS SVM:\n",
    "  - Funciona bem quando dados normais formam clusters compactos\n",
    "  - Sens√≠vel ao ajuste de hiperpar√¢metros\n",
    "  - Bom para identificar outliers\n",
    "\n",
    "‚Ä¢ ISOLATION FOREST:\n",
    "  - M√©todo ensemble usando florestas aleat√≥rias\n",
    "  - Efetivo para dados de alta dimensionalidade\n",
    "  - Menos sens√≠vel a hiperpar√¢metros\n",
    "\n",
    "‚Ä¢ AUTOENCODER:\n",
    "  - Abordagem de deep learning\n",
    "  - Aprende representa√ß√£o comprimida dos dados normais\n",
    "  - Erro de reconstru√ß√£o indica anomalias\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Recomenda√ß√µes para Aplica√ß√£o Industrial:\")\n",
    "print(\"\"\"\n",
    "1. Para m√°xima detec√ß√£o de anomalias (recall): One-Class SVM\n",
    "2. Para melhor balance geral: Isolation Forest\n",
    "3. Para m√°xima precis√£o: Autoencoder (com ajuste de threshold)\n",
    "4. Para produ√ß√£o: Considere ensemble dos tr√™s modelos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Salvando Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados em arquivo\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Criar timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Salvar modelos\n",
    "with open(f'models_vibration_anomaly_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'models': detector.models,\n",
    "        'scaler': detector.scaler,\n",
    "        'feature_names': detector.feature_names\n",
    "    }, f)\n",
    "\n",
    "# Salvar resultados em JSON\n",
    "results_json = {}\n",
    "for model_name, result in results.items():\n",
    "    results_json[model_name] = {\n",
    "        'accuracy': float(result['accuracy']),\n",
    "        'precision': float(result['precision']),\n",
    "        'recall': float(result['recall']),\n",
    "        'f1_score': float(result['f1_score']),\n",
    "        'specificity': float(result['specificity'])\n",
    "    }\n",
    "\n",
    "with open(f'results_vibration_anomaly_{timestamp}.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "# Salvar dados para an√°lise posterior\n",
    "df.to_csv(f'synthetic_vibration_data_{timestamp}.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Resultados salvos com timestamp: {timestamp}\")\n",
    "print(f\"   ‚Ä¢ Modelos: models_vibration_anomaly_{timestamp}.pkl\")\n",
    "print(f\"   ‚Ä¢ Resultados: results_vibration_anomaly_{timestamp}.json\")\n",
    "print(f\"   ‚Ä¢ Dados: synthetic_vibration_data_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclus√£o\n",
    "\n",
    "Este notebook implementou com sucesso um sistema completo de detec√ß√£o de anomalias para m√°quinas industriais usando dados de vibra√ß√£o de 3 eixos. \n",
    "\n",
    "### Principais Conquistas:\n",
    "\n",
    "1. **Gera√ß√£o de Dados Realistas**: Criamos dados sint√©ticos que simulam padr√µes reais de vibra√ß√£o\n",
    "2. **Extra√ß√£o de Features**: Implementamos 12 features relevantes em 3 canais\n",
    "3. **M√∫ltiplos Algoritmos**: Comparamos One-Class SVM, Isolation Forest e Autoencoder\n",
    "4. **Avalia√ß√£o Completa**: M√©tricas detalhadas e visualiza√ß√µes\n",
    "5. **Aplicabilidade Industrial**: C√≥digo pronto para adapta√ß√£o a dados reais\n",
    "\n",
    "### Pr√≥ximos Passos:\n",
    "\n",
    "- Testar com dados reais de m√°quinas industriais\n",
    "- Implementar ensemble dos modelos\n",
    "- Adicionar monitoramento em tempo real\n",
    "- Integrar com sistemas de manuten√ß√£o preditiva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}