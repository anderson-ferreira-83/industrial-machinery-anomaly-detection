{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de Anomalias em Máquinas Industriais usando Dados de Vibração 3-Eixos\n",
    "\n",
    "Este notebook implementa detecção de anomalias para máquinas industriais usando dados de vibração de 3 eixos, baseado no exemplo do MATLAB Predictive Maintenance Toolbox.\n",
    "\n",
    "## Metodologia\n",
    "\n",
    "Implementamos três abordagens diferentes de detecção de anomalias:\n",
    "1. **One-Class SVM**: Identifica anormalidades \"distantes\" dos dados normais\n",
    "2. **Isolation Forest**: Usa árvores de decisão para isolar observações\n",
    "3. **Autoencoder**: Rede neural que reconstrói dados normais com baixo erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n",
    "print(f\"TensorFlow versão: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definição da Classe Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VibrationAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detector de anomalias para máquinas industriais usando dados de vibração 3-eixos\n",
    "    Baseado no exemplo do MATLAB Predictive Maintenance Toolbox\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.feature_names = [\n",
    "            'Ch1_CrestFactor', 'Ch1_Kurtosis', 'Ch1_RMS', 'Ch1_StdDev',\n",
    "            'Ch2_Mean', 'Ch2_RMS', 'Ch2_Skewness', 'Ch2_StdDev',\n",
    "            'Ch3_CrestFactor', 'Ch3_SINAD', 'Ch3_SNR', 'Ch3_THD'\n",
    "        ]\n",
    "        print(\"Detector de Anomalias de Vibração inicializado!\")\n",
    "        print(f\"Features extraídas: {len(self.feature_names)}\")\n",
    "    \n",
    "    def generate_synthetic_data(self, n_normal=1000, n_anomaly=200):\n",
    "        \"\"\"\n",
    "        Gera dados sintéticos de vibração 3-eixos com comportamento normal e anômalo\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Dados normais (após manutenção)\n",
    "        normal_data = []\n",
    "        for i in range(n_normal):\n",
    "            # Features do Canal 1 (padrões normais de vibração)\n",
    "            ch1_crest = np.random.normal(3.2, 0.3)  # Fator de Crista\n",
    "            ch1_kurt = np.random.normal(3.1, 0.2)   # Curtose\n",
    "            ch1_rms = np.random.normal(0.15, 0.02)  # RMS\n",
    "            ch1_std = np.random.normal(0.12, 0.015) # Desvio Padrão\n",
    "            \n",
    "            # Features do Canal 2\n",
    "            ch2_mean = np.random.normal(0.001, 0.0005)  # Média\n",
    "            ch2_rms = np.random.normal(0.14, 0.018)     # RMS\n",
    "            ch2_skew = np.random.normal(0.05, 0.02)     # Assimetria\n",
    "            ch2_std = np.random.normal(0.11, 0.012)     # Desvio Padrão\n",
    "            \n",
    "            # Features do Canal 3\n",
    "            ch3_crest = np.random.normal(3.1, 0.25)  # Fator de Crista\n",
    "            ch3_sinad = np.random.normal(42, 2)      # SINAD\n",
    "            ch3_snr = np.random.normal(38, 1.5)      # SNR\n",
    "            ch3_thd = np.random.normal(0.08, 0.01)   # THD\n",
    "            \n",
    "            normal_data.append([\n",
    "                ch1_crest, ch1_kurt, ch1_rms, ch1_std,\n",
    "                ch2_mean, ch2_rms, ch2_skew, ch2_std,\n",
    "                ch3_crest, ch3_sinad, ch3_snr, ch3_thd\n",
    "            ])\n",
    "        \n",
    "        # Dados anômalos (antes da manutenção - condições degradadas)\n",
    "        anomaly_data = []\n",
    "        for i in range(n_anomaly):\n",
    "            # Features do Canal 1 (anômalas - vibrações maiores, padrões diferentes)\n",
    "            ch1_crest = np.random.normal(4.8, 0.5)    # Fator de crista maior\n",
    "            ch1_kurt = np.random.normal(5.2, 0.8)     # Curtose maior\n",
    "            ch1_rms = np.random.normal(0.35, 0.08)    # RMS maior\n",
    "            ch1_std = np.random.normal(0.28, 0.05)    # Desvio padrão maior\n",
    "            \n",
    "            # Features do Canal 2 (anômalas)\n",
    "            ch2_mean = np.random.normal(0.008, 0.003)  # Média maior\n",
    "            ch2_rms = np.random.normal(0.32, 0.06)     # RMS maior\n",
    "            ch2_skew = np.random.normal(0.25, 0.08)    # Assimetria maior\n",
    "            ch2_std = np.random.normal(0.25, 0.04)     # Desvio padrão maior\n",
    "            \n",
    "            # Features do Canal 3 (anômalas)\n",
    "            ch3_crest = np.random.normal(5.1, 0.6)   # Fator de crista maior\n",
    "            ch3_sinad = np.random.normal(28, 4)      # SINAD menor (mais distorção)\n",
    "            ch3_snr = np.random.normal(22, 3)        # SNR menor (mais ruído)\n",
    "            ch3_thd = np.random.normal(0.18, 0.03)   # THD maior (mais distorção)\n",
    "            \n",
    "            anomaly_data.append([\n",
    "                ch1_crest, ch1_kurt, ch1_rms, ch1_std,\n",
    "                ch2_mean, ch2_rms, ch2_skew, ch2_std,\n",
    "                ch3_crest, ch3_sinad, ch3_snr, ch3_thd\n",
    "            ])\n",
    "        \n",
    "        # Combinar dados\n",
    "        X_normal = np.array(normal_data)\n",
    "        X_anomaly = np.array(anomaly_data)\n",
    "        \n",
    "        # Criar rótulos (0 = normal, 1 = anomalia)\n",
    "        y_normal = np.zeros(n_normal)\n",
    "        y_anomaly = np.ones(n_anomaly)\n",
    "        \n",
    "        X = np.vstack([X_normal, X_anomaly])\n",
    "        y = np.hstack([y_normal, y_anomaly])\n",
    "        \n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame(X, columns=self.feature_names)\n",
    "        df['label'] = y\n",
    "        df['condition'] = ['Normal' if label == 0 else 'Anomalia' for label in y]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        Prepara dados para treinamento - divide e normaliza\n",
    "        \"\"\"\n",
    "        # Features e rótulos\n",
    "        X = df[self.feature_names].values\n",
    "        y = df['label'].values\n",
    "        \n",
    "        # Dividir dados\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Para detecção de anomalias, tipicamente treinamos apenas com dados normais\n",
    "        X_train_normal = X_train[y_train == 0]\n",
    "        \n",
    "        # Normalizar os dados\n",
    "        X_train_normal_scaled = self.scaler.fit_transform(X_train_normal)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_normal_scaled, X_test_scaled, y_test\n",
    "\n",
    "print(\"Classe VibrationAnomalyDetector definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geração e Exploração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o detector\n",
    "detector = VibrationAnomalyDetector()\n",
    "\n",
    "# Gerar dados sintéticos\n",
    "print(\"Gerando dados sintéticos de vibração...\")\n",
    "df = detector.generate_synthetic_data(n_normal=1000, n_anomaly=200)\n",
    "\n",
    "print(f\"\\nDados gerados:\")\n",
    "print(f\"- Total de amostras: {len(df)}\")\n",
    "print(f\"- Amostras normais: {len(df[df['label']==0])}\")\n",
    "print(f\"- Amostras anômalas: {len(df[df['label']==1])}\")\n",
    "print(f\"- Porcentagem de anomalias: {df['label'].mean()*100:.1f}%\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"\\nPrimeiras 5 amostras:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análise Exploratória dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas descritivas\n",
    "print(\"Estatísticas descritivas por condição:\")\n",
    "print(\"\\n=== DADOS NORMAIS ===\")\n",
    "display(df[df['condition']=='Normal'][detector.feature_names].describe())\n",
    "\n",
    "print(\"\\n=== DADOS ANÔMALOS ===\")\n",
    "display(df[df['condition']=='Anomalia'][detector.feature_names].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da distribuição dos dados\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(detector.feature_names):\n",
    "    # Histograma para cada feature\n",
    "    df[df['condition']=='Normal'][feature].hist(alpha=0.7, label='Normal', bins=30, ax=axes[i])\n",
    "    df[df['condition']=='Anomalia'][feature].hist(alpha=0.7, label='Anomalia', bins=30, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribuição das Features por Condição', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots para visualizar separação entre classes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Canal 1 - RMS vs Curtose\n",
    "axes[0,0].scatter(df[df['condition']=='Normal']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Normal']['Ch1_Kurtosis'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[0,0].scatter(df[df['condition']=='Anomalia']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Anomalia']['Ch1_Kurtosis'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[0,0].set_xlabel('Canal 1 - RMS')\n",
    "axes[0,0].set_ylabel('Canal 1 - Curtose')\n",
    "axes[0,0].set_title('Canal 1: RMS vs Curtose')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Canal 2 - RMS vs Desvio Padrão\n",
    "axes[0,1].scatter(df[df['condition']=='Normal']['Ch2_RMS'], \n",
    "                  df[df['condition']=='Normal']['Ch2_StdDev'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[0,1].scatter(df[df['condition']=='Anomalia']['Ch2_RMS'], \n",
    "                  df[df['condition']=='Anomalia']['Ch2_StdDev'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[0,1].set_xlabel('Canal 2 - RMS')\n",
    "axes[0,1].set_ylabel('Canal 2 - Desvio Padrão')\n",
    "axes[0,1].set_title('Canal 2: RMS vs Desvio Padrão')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Canal 3 - SNR vs THD\n",
    "axes[1,0].scatter(df[df['condition']=='Normal']['Ch3_SNR'], \n",
    "                  df[df['condition']=='Normal']['Ch3_THD'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[1,0].scatter(df[df['condition']=='Anomalia']['Ch3_SNR'], \n",
    "                  df[df['condition']=='Anomalia']['Ch3_THD'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[1,0].set_xlabel('Canal 3 - SNR')\n",
    "axes[1,0].set_ylabel('Canal 3 - THD')\n",
    "axes[1,0].set_title('Canal 3: SNR vs THD')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Correlação entre canais\n",
    "axes[1,1].scatter(df[df['condition']=='Normal']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Normal']['Ch2_RMS'], \n",
    "                  alpha=0.6, label='Normal', s=50)\n",
    "axes[1,1].scatter(df[df['condition']=='Anomalia']['Ch1_RMS'], \n",
    "                  df[df['condition']=='Anomalia']['Ch2_RMS'], \n",
    "                  alpha=0.8, label='Anomalia', s=50)\n",
    "axes[1,1].set_xlabel('Canal 1 - RMS')\n",
    "axes[1,1].set_ylabel('Canal 2 - RMS')\n",
    "axes[1,1].set_title('Correlação entre Canais: RMS')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para treinamento\n",
    "print(\"Preparando dados para treinamento...\")\n",
    "X_train_normal, X_test, y_test = detector.prepare_data(df)\n",
    "\n",
    "print(f\"\\nDivisão dos dados:\")\n",
    "print(f\"- Conjunto de treino (apenas normais): {X_train_normal.shape[0]} amostras\")\n",
    "print(f\"- Conjunto de teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"- Features: {X_train_normal.shape[1]}\")\n",
    "print(f\"- Anomalias no teste: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# Verificar normalização\n",
    "print(f\"\\nVerificação da normalização (conjunto de treino):\")\n",
    "print(f\"- Média: {X_train_normal.mean(axis=0)[:3]:.4f}... (primeiras 3 features)\")\n",
    "print(f\"- Desvio padrão: {X_train_normal.std(axis=0)[:3]:.4f}... (primeiras 3 features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementação dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_class_svm(detector, X_train_normal):\n",
    "    \"\"\"\n",
    "    Treina One-Class SVM para detecção de anomalias\n",
    "    \"\"\"\n",
    "    print(\"Treinando One-Class SVM...\")\n",
    "    \n",
    "    # One-Class SVM\n",
    "    oc_svm = OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')\n",
    "    oc_svm.fit(X_train_normal)\n",
    "    \n",
    "    detector.models['OneClassSVM'] = oc_svm\n",
    "    print(\"✅ Treinamento do One-Class SVM concluído.\")\n",
    "    \n",
    "    # Informações do modelo\n",
    "    print(f\"   - Parâmetro nu: {oc_svm.nu}\")\n",
    "    print(f\"   - Kernel: {oc_svm.kernel}\")\n",
    "    print(f\"   - Gamma: {oc_svm.gamma}\")\n",
    "    print(f\"   - Vetores de suporte: {len(oc_svm.support_vectors_)}\")\n",
    "\n",
    "# Treinar One-Class SVM\n",
    "train_one_class_svm(detector, X_train_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_isolation_forest(detector, X_train_normal):\n",
    "    \"\"\"\n",
    "    Treina Isolation Forest para detecção de anomalias\n",
    "    \"\"\"\n",
    "    print(\"Treinando Isolation Forest...\")\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "    iso_forest.fit(X_train_normal)\n",
    "    \n",
    "    detector.models['IsolationForest'] = iso_forest\n",
    "    print(\"✅ Treinamento do Isolation Forest concluído.\")\n",
    "    \n",
    "    # Informações do modelo\n",
    "    print(f\"   - Contaminação esperada: {iso_forest.contamination}\")\n",
    "    print(f\"   - Número de árvores: {iso_forest.n_estimators}\")\n",
    "    print(f\"   - Max samples: {iso_forest.max_samples}\")\n",
    "\n",
    "# Treinar Isolation Forest\n",
    "train_isolation_forest(detector, X_train_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Autoencoder Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Constrói rede neural autoencoder\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder = keras.Sequential([\n",
    "        keras.layers.Dense(8, activation='relu', input_shape=(input_dim,), name='encoder_layer1'),\n",
    "        keras.layers.Dense(4, activation='relu', name='encoder_layer2'),\n",
    "        keras.layers.Dense(2, activation='relu', name='bottleneck')  # Camada gargalo\n",
    "    ], name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = keras.Sequential([\n",
    "        keras.layers.Dense(4, activation='relu', input_shape=(2,), name='decoder_layer1'),\n",
    "        keras.layers.Dense(8, activation='relu', name='decoder_layer2'),\n",
    "        keras.layers.Dense(input_dim, activation='linear', name='output')\n",
    "    ], name='decoder')\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = keras.Sequential([encoder, decoder], name='autoencoder')\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "def train_autoencoder(detector, X_train_normal):\n",
    "    \"\"\"\n",
    "    Treina autoencoder para detecção de anomalias\n",
    "    \"\"\"\n",
    "    print(\"Treinando Autoencoder...\")\n",
    "    \n",
    "    # Construir e treinar autoencoder\n",
    "    autoencoder, encoder, decoder = build_autoencoder(X_train_normal.shape[1])\n",
    "    \n",
    "    # Mostrar arquitetura\n",
    "    print(\"\\nArquitetura do Autoencoder:\")\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    # Treinar apenas com dados normais\n",
    "    history = autoencoder.fit(\n",
    "        X_train_normal, X_train_normal,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    detector.models['Autoencoder'] = autoencoder\n",
    "    detector.models['Encoder'] = encoder\n",
    "    detector.models['Decoder'] = decoder\n",
    "    \n",
    "    print(\"✅ Treinamento do Autoencoder concluído.\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Treinar Autoencoder\n",
    "history = train_autoencoder(detector, X_train_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar histórico de treinamento do Autoencoder\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Treino', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validação', linewidth=2)\n",
    "axes[0].set_title('Função de Perda do Autoencoder')\n",
    "axes[0].set_xlabel('Época')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Treino', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validação', linewidth=2)\n",
    "axes[1].set_title('Erro Absoluto Médio do Autoencoder')\n",
    "axes[1].set_xlabel('Época')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Perda final de treino: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Perda final de validação: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predições e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomalies(detector, X_test):\n",
    "    \"\"\"\n",
    "    Prediz anomalias usando todos os modelos treinados\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    # Predições do One-Class SVM\n",
    "    if 'OneClassSVM' in detector.models:\n",
    "        print(\"Fazendo predições com One-Class SVM...\")\n",
    "        oc_pred = detector.models['OneClassSVM'].predict(X_test)\n",
    "        # Converter -1 (anomalia) para 1, e 1 (normal) para 0\n",
    "        predictions['OneClassSVM'] = (oc_pred == -1).astype(int)\n",
    "        print(f\"  Anomalias detectadas: {predictions['OneClassSVM'].sum()}\")\n",
    "    \n",
    "    # Predições do Isolation Forest\n",
    "    if 'IsolationForest' in detector.models:\n",
    "        print(\"Fazendo predições com Isolation Forest...\")\n",
    "        iso_pred = detector.models['IsolationForest'].predict(X_test)\n",
    "        # Converter -1 (anomalia) para 1, e 1 (normal) para 0\n",
    "        predictions['IsolationForest'] = (iso_pred == -1).astype(int)\n",
    "        print(f\"  Anomalias detectadas: {predictions['IsolationForest'].sum()}\")\n",
    "    \n",
    "    # Predições do Autoencoder\n",
    "    if 'Autoencoder' in detector.models:\n",
    "        print(\"Fazendo predições com Autoencoder...\")\n",
    "        # Reconstruir dados\n",
    "        reconstructed = detector.models['Autoencoder'].predict(X_test, verbose=0)\n",
    "        # Calcular erro de reconstrução\n",
    "        mse = np.mean(np.square(X_test - reconstructed), axis=1)\n",
    "        # Usar threshold para classificar (percentil 95 dos erros de reconstrução)\n",
    "        threshold = np.percentile(mse, 95)\n",
    "        predictions['Autoencoder'] = (mse > threshold).astype(int)\n",
    "        print(f\"  Threshold de erro: {threshold:.4f}\")\n",
    "        print(f\"  Anomalias detectadas: {predictions['Autoencoder'].sum()}\")\n",
    "        \n",
    "        # Armazenar erros de reconstrução para análise\n",
    "        detector.reconstruction_errors = mse\n",
    "        detector.threshold = threshold\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Fazer predições\n",
    "print(\"Fazendo predições com todos os modelos...\")\n",
    "predictions = predict_anomalies(detector, X_test)\n",
    "print(f\"\\nTotal de anomalias reais no teste: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avaliação Detalhada dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_true, predictions):\n",
    "    \"\"\"\n",
    "    Avalia performance dos modelos\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, y_pred in predictions.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📊 RESULTADOS DO {model_name.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Relatório de classificação\n",
    "        report = classification_report(y_true, y_pred, target_names=['Normal', 'Anomalia'])\n",
    "        print(report)\n",
    "        \n",
    "        # Matriz de confusão\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(f\"\\nMatriz de Confusão:\")\n",
    "        print(f\"         Predito\")\n",
    "        print(f\"       Normal Anomalia\")\n",
    "        print(f\"Normal   {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "        print(f\"Anomalia {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "        \n",
    "        # Calcular métricas\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n📈 Métricas Detalhadas:\")\n",
    "        print(f\"   • Acurácia: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "        print(f\"   • Precisão: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "        print(f\"   • Recall (Sensibilidade): {recall:.3f} ({recall*100:.1f}%)\")\n",
    "        print(f\"   • Especificidade: {specificity:.3f} ({specificity*100:.1f}%)\")\n",
    "        print(f\"   • F1-Score: {f1:.3f}\")\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'specificity': specificity,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm,\n",
    "            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Avaliar todos os modelos\n",
    "results = evaluate_models(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizações dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação visual dos modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Preparar dados para comparação\n",
    "model_names = list(results.keys())\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "metric_labels = ['Acurácia', 'Precisão', 'Recall', 'F1-Score']\n",
    "\n",
    "# Gráfico de barras comparativo\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    values = [results[model][metric] for model in model_names]\n",
    "    bars = ax.bar(x, values, width=0.6, alpha=0.8)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{label} por Modelo', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão visual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    cm = result['confusion_matrix']\n",
    "    \n",
    "    # Heatmap da matriz de confusão\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Anomalia'],\n",
    "                yticklabels=['Normal', 'Anomalia'],\n",
    "                ax=axes[i], cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    axes[i].set_title(f'{model_name}\\nAcurácia: {result[\"accuracy\"]:.3f}', \n",
    "                      fontweight='bold')\n",
    "    axes[i].set_ylabel('Valores Reais')\n",
    "    axes[i].set_xlabel('Predições')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise dos erros de reconstrução do Autoencoder\n",
    "if hasattr(detector, 'reconstruction_errors'):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histograma dos erros de reconstrução\n",
    "    normal_errors = detector.reconstruction_errors[y_test == 0]\n",
    "    anomaly_errors = detector.reconstruction_errors[y_test == 1]\n",
    "    \n",
    "    axes[0].hist(normal_errors, bins=30, alpha=0.7, label='Normal', density=True)\n",
    "    axes[0].hist(anomaly_errors, bins=30, alpha=0.7, label='Anomalia', density=True)\n",
    "    axes[0].axvline(detector.threshold, color='red', linestyle='--', \n",
    "                    label=f'Threshold ({detector.threshold:.4f})')\n",
    "    axes[0].set_xlabel('Erro de Reconstrução (MSE)')\n",
    "    axes[0].set_ylabel('Densidade')\n",
    "    axes[0].set_title('Distribuição dos Erros de Reconstrução')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot dos erros\n",
    "    data_for_box = [normal_errors, anomaly_errors]\n",
    "    axes[1].boxplot(data_for_box, labels=['Normal', 'Anomalia'])\n",
    "    axes[1].axhline(detector.threshold, color='red', linestyle='--', \n",
    "                    label=f'Threshold ({detector.threshold:.4f})')\n",
    "    axes[1].set_ylabel('Erro de Reconstrução (MSE)')\n",
    "    axes[1].set_title('Box Plot dos Erros de Reconstrução')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Estatísticas dos erros de reconstrução:\")\n",
    "    print(f\"Normal - Média: {normal_errors.mean():.4f}, Desvio: {normal_errors.std():.4f}\")\n",
    "    print(f\"Anomalia - Média: {anomaly_errors.mean():.4f}, Desvio: {anomaly_errors.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo Final e Comparação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela resumo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 RESUMO FINAL - COMPARAÇÃO DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Criar DataFrame para comparação\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name,\n",
    "        'Acurácia': f\"{result['accuracy']:.3f}\",\n",
    "        'Precisão': f\"{result['precision']:.3f}\",\n",
    "        'Recall': f\"{result['recall']:.3f}\",\n",
    "        'F1-Score': f\"{result['f1_score']:.3f}\",\n",
    "        'Especificidade': f\"{result['specificity']:.3f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# Identificar melhor modelo por métrica\n",
    "print(\"\\n🏆 MELHORES MODELOS POR MÉTRICA:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "    best_model = max(results.keys(), key=lambda x: results[x][metric])\n",
    "    best_value = results[best_model][metric]\n",
    "    metric_name = {'accuracy': 'Acurácia', 'precision': 'Precisão', \n",
    "                   'recall': 'Recall', 'f1_score': 'F1-Score'}[metric]\n",
    "    print(f\"   • {metric_name}: {best_model} ({best_value:.3f})\")\n",
    "\n",
    "# Análise interpretativa\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 ANÁLISE INTERPRETATIVA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔍 Características dos Modelos:\")\n",
    "print(\"\"\"\n",
    "• ONE-CLASS SVM:\n",
    "  - Funciona bem quando dados normais formam clusters compactos\n",
    "  - Sensível ao ajuste de hiperparâmetros\n",
    "  - Bom para identificar outliers\n",
    "\n",
    "• ISOLATION FOREST:\n",
    "  - Método ensemble usando florestas aleatórias\n",
    "  - Efetivo para dados de alta dimensionalidade\n",
    "  - Menos sensível a hiperparâmetros\n",
    "\n",
    "• AUTOENCODER:\n",
    "  - Abordagem de deep learning\n",
    "  - Aprende representação comprimida dos dados normais\n",
    "  - Erro de reconstrução indica anomalias\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n💡 Recomendações para Aplicação Industrial:\")\n",
    "print(\"\"\"\n",
    "1. Para máxima detecção de anomalias (recall): One-Class SVM\n",
    "2. Para melhor balance geral: Isolation Forest\n",
    "3. Para máxima precisão: Autoencoder (com ajuste de threshold)\n",
    "4. Para produção: Considere ensemble dos três modelos\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Salvando Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados em arquivo\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Criar timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Salvar modelos\n",
    "with open(f'models_vibration_anomaly_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'models': detector.models,\n",
    "        'scaler': detector.scaler,\n",
    "        'feature_names': detector.feature_names\n",
    "    }, f)\n",
    "\n",
    "# Salvar resultados em JSON\n",
    "results_json = {}\n",
    "for model_name, result in results.items():\n",
    "    results_json[model_name] = {\n",
    "        'accuracy': float(result['accuracy']),\n",
    "        'precision': float(result['precision']),\n",
    "        'recall': float(result['recall']),\n",
    "        'f1_score': float(result['f1_score']),\n",
    "        'specificity': float(result['specificity'])\n",
    "    }\n",
    "\n",
    "with open(f'results_vibration_anomaly_{timestamp}.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "# Salvar dados para análise posterior\n",
    "df.to_csv(f'synthetic_vibration_data_{timestamp}.csv', index=False)\n",
    "\n",
    "print(f\"✅ Resultados salvos com timestamp: {timestamp}\")\n",
    "print(f\"   • Modelos: models_vibration_anomaly_{timestamp}.pkl\")\n",
    "print(f\"   • Resultados: results_vibration_anomaly_{timestamp}.json\")\n",
    "print(f\"   • Dados: synthetic_vibration_data_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Conclusão\n",
    "\n",
    "Este notebook implementou com sucesso um sistema completo de detecção de anomalias para máquinas industriais usando dados de vibração de 3 eixos. \n",
    "\n",
    "### Principais Conquistas:\n",
    "\n",
    "1. **Geração de Dados Realistas**: Criamos dados sintéticos que simulam padrões reais de vibração\n",
    "2. **Extração de Features**: Implementamos 12 features relevantes em 3 canais\n",
    "3. **Múltiplos Algoritmos**: Comparamos One-Class SVM, Isolation Forest e Autoencoder\n",
    "4. **Avaliação Completa**: Métricas detalhadas e visualizações\n",
    "5. **Aplicabilidade Industrial**: Código pronto para adaptação a dados reais\n",
    "\n",
    "### Próximos Passos:\n",
    "\n",
    "- Testar com dados reais de máquinas industriais\n",
    "- Implementar ensemble dos modelos\n",
    "- Adicionar monitoramento em tempo real\n",
    "- Integrar com sistemas de manutenção preditiva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}